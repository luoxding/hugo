<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'mesh' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/Mesh/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&amp;list_id=177877&amp;order=changeddate%20DESC%2Cpriority%2Cbug_severity&amp;product=MLIR&amp;query_format=specific">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'mesh' Dialect</h1><p>The <code>mesh</code> dialect contains a set of attributes, operations and interfaces that
are useful for representing sharding and communication on a device mesh
cluster.</p><p><nav id=TableOfContents><ul><li><a href=#collective-communication-operations>Collective Communication Operations</a><ul><li><a href=#device-groups>Device groups</a></li><li><a href=#in-group-device>In-group Device</a></li></ul></li><li><a href=#operations>Operations</a></li><li><a href=#operations-1>Operations</a><ul><li><a href=#meshall_gather-meshallgatherop><code>mesh.all_gather</code> (mesh::AllGatherOp)</a></li><li><a href=#meshall_reduce-meshallreduceop><code>mesh.all_reduce</code> (mesh::AllReduceOp)</a></li><li><a href=#meshall_to_all-meshalltoallop><code>mesh.all_to_all</code> (mesh::AllToAllOp)</a></li><li><a href=#meshbroadcast-meshbroadcastop><code>mesh.broadcast</code> (mesh::BroadcastOp)</a></li><li><a href=#meshcluster-meshclusterop><code>mesh.cluster</code> (mesh::ClusterOp)</a></li><li><a href=#meshgather-meshgatherop><code>mesh.gather</code> (mesh::GatherOp)</a></li><li><a href=#meshrecv-meshrecvop><code>mesh.recv</code> (mesh::RecvOp)</a></li><li><a href=#meshreduce-meshreduceop><code>mesh.reduce</code> (mesh::ReduceOp)</a></li><li><a href=#meshreduce_scatter-meshreducescatterop><code>mesh.reduce_scatter</code> (mesh::ReduceScatterOp)</a></li><li><a href=#meshscatter-meshscatterop><code>mesh.scatter</code> (mesh::ScatterOp)</a></li><li><a href=#meshsend-meshsendop><code>mesh.send</code> (mesh::SendOp)</a></li><li><a href=#meshshard-meshshardop><code>mesh.shard</code> (mesh::ShardOp)</a></li><li><a href=#meshshift-meshshiftop><code>mesh.shift</code> (mesh::ShiftOp)</a></li></ul></li><li><a href=#attributes-13>Attributes</a><ul><li><a href=#meshshardingattr>MeshShardingAttr</a></li><li><a href=#partialattr>PartialAttr</a></li></ul></li><li><a href=#attributes-14>Attributes</a></li></ul></nav><h2 id=collective-communication-operations>Collective Communication Operations&nbsp;<a class=headline-hash href=#collective-communication-operations>¶</a></h2><p>There are a number of operations in the Mesh dialect to facilitate
communication between devices in a mesh.
It is assumed that the user is familiar with collective operations.
<a href=https://en.wikipedia.org/wiki/Collective_operation>Wikipedia</a> has a good
explanation.
The main addition is that the collectives in this dialect have mesh
semantics.</p><h3 id=device-groups>Device groups&nbsp;<a class=headline-hash href=#device-groups>¶</a></h3><p>The operation attributes <code>mesh</code> and <code>mesh_axes</code> specifies a list of device mesh
axes that partition the devices into disjoint groups.
The collective operation is performed between devices in the same group.
Devices that have the same coordinates outside of axes <code>mesh_axes</code> are in the
same group.
A group is described by its multi-index along the axes outside of <code>mesh_axes</code>.
For example if we have a device mesh of size <code>2x3x4x5</code> and the partition mesh
axes list is <code>[0, 1]</code> then devices are partitioned into the groups
<code>{ { (i, j, k, m) | 0&lt;=i&lt;2, 0&lt;=j&lt;3 } | 0&lt;=k&lt;4, 0&lt;=m&lt;5 }</code>.
The device groups would be <code>{ (k, m) | 0&lt;=k&lt;4, 0&lt;=m&lt;5 }</code>.
Devices (1, 0, 2, 3) and (1, 1, 2, 3) will be in the same group.
Device (1, 0, 2, 4) will be in another group.
Some collective operations like all-to-all and all-gather care about the
order of devices.
The order of device in a device group is induced by the order of axes in
<code>mesh_axes</code>.
The axes are ordered from outer to inner.
If we have an axis list <code>[3, 1]</code> then device <code>(i, 1, k, 0)</code> will precede
both devices <code>(i, 0, k, 1)</code> and <code>(i, 2, k, 0)</code>.</p><h3 id=in-group-device>In-group Device&nbsp;<a class=headline-hash href=#in-group-device>¶</a></h3><p>Some operations like <code>broadcast</code>, <code>scatter</code> and <code>send</code> specify devices in each
device-group.
These devices are represented with their multi-index over the mesh axes that
are not constant within a device group.
These are the axes specified by <code>mesh_axes</code> attribute.</p><p>For Example on a 3D mesh an operation with <code>mesh_axes = [0, 2]</code> would specify
an in-group device with <code>(i, j)</code>. Then for each group with index <code>g</code> on the
second axis, the in-group device would be <code>(i, g, j)</code>.</p><h2 id=operations>Operations&nbsp;<a class=headline-hash href=#operations>¶</a></h2><p>See
<a href=mlir/docs/Dialects/Mesh.md>Mesh dialect documentation</a>.</p><p><nav id=TableOfContents><ul><li><a href=#collective-communication-operations>Collective Communication Operations</a><ul><li><a href=#device-groups>Device groups</a></li><li><a href=#in-group-device>In-group Device</a></li></ul></li><li><a href=#operations>Operations</a></li><li><a href=#operations-1>Operations</a><ul><li><a href=#meshall_gather-meshallgatherop><code>mesh.all_gather</code> (mesh::AllGatherOp)</a></li><li><a href=#meshall_reduce-meshallreduceop><code>mesh.all_reduce</code> (mesh::AllReduceOp)</a></li><li><a href=#meshall_to_all-meshalltoallop><code>mesh.all_to_all</code> (mesh::AllToAllOp)</a></li><li><a href=#meshbroadcast-meshbroadcastop><code>mesh.broadcast</code> (mesh::BroadcastOp)</a></li><li><a href=#meshcluster-meshclusterop><code>mesh.cluster</code> (mesh::ClusterOp)</a></li><li><a href=#meshgather-meshgatherop><code>mesh.gather</code> (mesh::GatherOp)</a></li><li><a href=#meshrecv-meshrecvop><code>mesh.recv</code> (mesh::RecvOp)</a></li><li><a href=#meshreduce-meshreduceop><code>mesh.reduce</code> (mesh::ReduceOp)</a></li><li><a href=#meshreduce_scatter-meshreducescatterop><code>mesh.reduce_scatter</code> (mesh::ReduceScatterOp)</a></li><li><a href=#meshscatter-meshscatterop><code>mesh.scatter</code> (mesh::ScatterOp)</a></li><li><a href=#meshsend-meshsendop><code>mesh.send</code> (mesh::SendOp)</a></li><li><a href=#meshshard-meshshardop><code>mesh.shard</code> (mesh::ShardOp)</a></li><li><a href=#meshshift-meshshiftop><code>mesh.shift</code> (mesh::ShiftOp)</a></li></ul></li><li><a href=#attributes-13>Attributes</a><ul><li><a href=#meshshardingattr>MeshShardingAttr</a></li><li><a href=#partialattr>PartialAttr</a></li></ul></li><li><a href=#attributes-14>Attributes</a></li></ul></nav><h2 id=operations-1>Operations&nbsp;<a class=headline-hash href=#operations-1>¶</a></h2><p><a href=https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Mesh/IR/MeshOps.td>source</a></p><h3 id=meshall_gather-meshallgatherop><code>mesh.all_gather</code> (mesh::AllGatherOp)&nbsp;<a class=headline-hash href=#meshall_gather-meshallgatherop>¶</a></h3><p><em>All-gather over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.all_gather` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)? `gather_axis` `=` $gather_axis
              attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>Gathers along the <code>gather_axis</code> tensor axis.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>mesh<span class=p>.</span>cluster <span class=nf>@mesh0</span><span class=p>(</span><span class=nl>rank =</span> <span class=m>2</span><span class=p>,</span> <span class=nl>dim_sizes =</span> <span class=m>2x2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=nv>%1</span> <span class=p>=</span> mesh<span class=p>.</span>all_gather <span class=nv>%0</span> on <span class=nf>@mesh0</span> <span class=nl>mesh_axes =</span> <span class=p>[</span><span class=m>1</span><span class=p>]</span> <span class=nl>gather_axis =</span> <span class=m>1</span>
</span></span><span class=line><span class=cl>  <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>i8</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x4x</span><span class=k>i8</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Input:</p><pre tabindex=0><code>                 +-------+-------+
device (0, 0) -&gt; |  1  2 |  5  6 | &lt;- device (0, 1)
                 |  3  4 |  7  8 |
                 +-------+-------+
device (1, 0) -&gt; |  9 10 | 13 14 | &lt;- device (1, 1)
                 | 11 12 | 15 16 |
                 +-------+-------+
</code></pre><p>Result:</p><pre tabindex=0><code>gather tensor
axis 1
------------&gt;
+-------------+
|  1  2  5  6 | &lt;- devices (0, 0) and (0, 1)
|  3  4  7  8 |
+-------------+
|  9 10 13 14 | &lt;- devices (1, 0) and (1, 1)
| 11 12 15 16 |
+-------------+
</code></pre><p>Traits: SameOperandsAndResultElementType, SameOperandsAndResultRank</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>gather_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr></table><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h3 id=meshall_reduce-meshallreduceop><code>mesh.all_reduce</code> (mesh::AllReduceOp)&nbsp;<a class=headline-hash href=#meshall_reduce-meshallreduceop>¶</a></h3><p><em>All-reduce over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.all_reduce` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)? (`reduction` `=` $reduction^)?
              attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>The accumulation element type is specified by the result type and
it does not need to match the input element type.
The input element is converted to the result element type before
performing the reduction.</p><p>Attributes:
<code>reduction</code>: Indicates the reduction method.</p><p>Example:</p><pre tabindex=0><code>%1 = mesh.all_reduce %0 on @mesh0 mesh_axes = [1, 0] reduction = &lt;max&gt;
  : tensor&lt;3x4xf32&gt; -&gt; tensor&lt;3x4xf64&gt;
</code></pre><p>Traits: SameOperandsAndResultShape</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-1>Attributes:&nbsp;<a class=headline-hash href=#attributes-1>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>reduction</code></td><td>::mlir::mesh::PartialAttr</td><td><details><summary>partial type of a distributed tensor</summary><p>Enum cases:</p><ul><li>sum (<code>Sum</code>)</li><li>max (<code>Max</code>)</li><li>min (<code>Min</code>)</li><li>generic (<code>Generic</code>)</li></ul></details></td></tr></table><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshall_to_all-meshalltoallop><code>mesh.all_to_all</code> (mesh::AllToAllOp)&nbsp;<a class=headline-hash href=#meshall_to_all-meshalltoallop>¶</a></h3><p><em>All-to-all over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.all_to_all` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `split_axis` `=` $split_axis
              `concat_axis` `=` $concat_axis
              attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>Performs an all-to-all on tensor pieces split along <code>split_axis</code>.
The resulting pieces are concatenated along <code>concat_axis</code> on ech device.</p><p>Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 1, dim_sizes = 3)
...
%1 = mesh.all_to_all %0 on @mesh0 mesh_axes = [0]
  split_axis = 0 concat_axis = 0
  : tensor&lt;3x2xi8&gt; -&gt; tensor&lt;3x2xi8&gt;
</code></pre><p>Input:</p><pre tabindex=0><code> device  device  device
 (0)     (1)     (2)
+-------+-------+-------+  | split and concat along
| 11 12 | 21 22 | 31 32 |  | tensor axis 0
| 13 14 | 23 24 | 33 34 |  ↓
| 15 16 | 25 26 | 35 36 |
+-------+-------+-------+
</code></pre><p>Result:</p><pre tabindex=0><code> device  device  device
 (0)     (1)     (2)
+-------+-------+-------+
| 11 12 | 13 14 | 15 16 |
| 21 22 | 23 24 | 25 26 |
| 31 32 | 33 34 | 35 36 |
+-------+-------+-------+
</code></pre><p>Traits: SameOperandsAndResultElementType, SameOperandsAndResultRank</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-2>Attributes:&nbsp;<a class=headline-hash href=#attributes-2>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>split_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td><code>concat_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr></table><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h3 id=meshbroadcast-meshbroadcastop><code>mesh.broadcast</code> (mesh::BroadcastOp)&nbsp;<a class=headline-hash href=#meshbroadcast-meshbroadcastop>¶</a></h3><p><em>Broadcast over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.broadcast` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `root` `=` custom&lt;DynamicIndexList&gt;($root_dynamic, $root)
              attr-dict `:` functional-type(operands, results)
</code></pre><p>Broadcast the tensor on <code>root</code> to all devices in each respective group.
The operation broadcasts along mesh axes <code>mesh_axes</code>.
The <code>root</code> device specifies the in-group multi-index that is broadcast to
all other devices in the group.</p><p>Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 2, dim_sizes = [2, 2])

%1 = mesh.broadcast %0 on @mesh0
  mesh_axes = [0]
  root = [0]
  : (tensor&lt;2xi8&gt;) -&gt; tensor&lt;2xi8&gt;
</code></pre><p>Input:</p><pre tabindex=0><code>                 +-------+-------+                   | broadcast
device (0, 0) -&gt; |  1  2 |  3  4 | &lt;- device (0, 1)  | along axis 0
                 +-------+-------+                   ↓
device (1, 0) -&gt; |       |       | &lt;- device (1, 1) 
                 +-------+-------+
</code></pre><p>Output:</p><pre tabindex=0><code>                 +-------+-------+
device (0, 0) -&gt; |  1  2 |  3  4 | &lt;- device (0, 1)
                 +-------+-------+
device (1, 0) -&gt; |  1  2 |  3  4 | &lt;- device (1, 1)
                 +-------+-------+
</code></pre><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-3>Attributes:&nbsp;<a class=headline-hash href=#attributes-3>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>root</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>ranked tensor of any type values</td></tr><tr><td style=text-align:center><code>root_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshcluster-meshclusterop><code>mesh.cluster</code> (mesh::ClusterOp)&nbsp;<a class=headline-hash href=#meshcluster-meshclusterop>¶</a></h3><p><em>Representing a mesh cluster</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.cluster` $sym_name `(` `rank` `=` $rank (`,` `dim_sizes` `=` custom&lt;DimensionList&gt;($dim_sizes)^)? `)`
              attr-dict
</code></pre><p>The mesh.cluster operation is a symbol operation that identifies a specific
mesh cluster. The operation has three attributes:</p><ol><li><p><code>sym_name</code>: This attribute uniquely identifies the name of the mesh
cluster. This name serves as a symbolic reference to the cluster throughout
the MLIR module, allowing for consistent referencing and easier debugging.</p></li><li><p><code>rank</code>: This attribute specifies the number of axes of the cluster. The
rank indicates the dimensionality of the mesh cluster and can be used to
determine the layout and the addressing space of the computation distributed
across the mesh.</p></li><li><p><code>dim_sizes</code>: This attribute represents the shape of the device cluster.
It uses the same notation as a tensor shape. Also allowing for dynamic
dimensions.
This flexibility allows for dynamic device assignment or configurations
where the exact number of devices might not be determined during compile
time.
For example <code>2x?x4</code>.</p></li></ol><p>Example:</p><pre tabindex=0><code>// A device mesh cluster with 3 axes, the total device number is 4 * 8 * 12
// The dimension sizes are 4, 8, 12 
mesh.cluster @mesh0(rank = 3, dim_sizes = 4x8x12)

// A device mesh cluster with 2 axes, the total device number is unknown
// The first dimension size is 4 and the second is unknown
mesh.cluster @mesh1(rank = 2, dim_sizes = 4)

// A device mesh cluster with 2 axes, the total device number is unknown
// The first dimension size is unknown and the second is 4
mesh.cluster @mesh2(rank = 2, dim_sizes = ?x4)

// A device mesh cluster with 2 axes, the number of devices along both axes
// is unknown
mesh.cluster @mesh3(rank = 2)

// Used in the mesh sharding attribute to extend the standard tensor to
// distributed
tensor&lt;4x8xf32, #mesh.shard&lt;@mesh0, [[0]]&gt;&gt;
</code></pre><p>Interfaces: Symbol</p><h4 id=attributes-4>Attributes:&nbsp;<a class=headline-hash href=#attributes-4>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>sym_name</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr><tr><td><code>rank</code></td><td>::mlir::IntegerAttr</td><td>64-bit signless integer attribute</td></tr><tr><td><code>dim_sizes</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h3 id=meshgather-meshgatherop><code>mesh.gather</code> (mesh::GatherOp)&nbsp;<a class=headline-hash href=#meshgather-meshgatherop>¶</a></h3><p><em>Gather over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.gather` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `gather_axis` `=` $gather_axis
              `root` `=` custom&lt;DynamicIndexList&gt;($root_dynamic, $root)
              attr-dict `:` functional-type(operands, results)
</code></pre><p>Gathers on device <code>root</code> along the <code>gather_axis</code> tensor axis.
<code>root</code> specifies the coordinates of a device along <code>mesh_axes</code>.
It uniquely identifies the root device for each device group.
The result tensor on non-root devices is undefined.
Using it will result in undefined behavior.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>mesh<span class=p>.</span>cluster <span class=nf>@mesh0</span><span class=p>(</span><span class=nl>rank =</span> <span class=m>2</span><span class=p>,</span> <span class=nl>dim_sizes =</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=nv>%1</span> <span class=p>=</span> mesh<span class=p>.</span>gather <span class=nv>%0</span> on <span class=nf>@mesh0</span> <span class=nl>mesh_axes =</span> <span class=p>[</span><span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=nl>gather_axis =</span> <span class=m>1</span> <span class=nl>root =</span> <span class=p>[</span><span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>i8</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x4x</span><span class=k>i8</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Input:</p><pre tabindex=0><code>                  gather tensor
                  axis 1
                  ------------&gt;
                 +-------+-------+
device (0, 0) -&gt; |  1  2 |  5  6 | &lt;- device (0, 1)
                 |  3  4 |  7  8 |
                 +-------+-------+
device (1, 0) -&gt; |  9 10 | 13 14 | &lt;- device (1, 1)
                 | 11 12 | 15 16 |
                 +-------+-------+
</code></pre><p>Result:</p><pre tabindex=0><code>+-------------+
|  1  2  5  6 | &lt;- devices (0, 1)
|  3  4  7  8 |
+-------------+
|  9 10 13 14 | &lt;- devices (1, 1)
| 11 12 15 16 |
+-------------+
</code></pre><p>Devices <code>(0, 0)</code> and <code>(1, 0)</code> have undefined result.</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-5>Attributes:&nbsp;<a class=headline-hash href=#attributes-5>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>gather_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td><code>root</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr><tr><td style=text-align:center><code>root_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h3 id=meshrecv-meshrecvop><code>mesh.recv</code> (mesh::RecvOp)&nbsp;<a class=headline-hash href=#meshrecv-meshrecvop>¶</a></h3><p><em>Send over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.recv` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              (`source` `=` custom&lt;DynamicIndexList&gt;($source_dynamic, $source)^)?
              attr-dict `:` functional-type(operands, results)
</code></pre><p>Receive from a device within a device group.</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-6>Attributes:&nbsp;<a class=headline-hash href=#attributes-6>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>source</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr><tr><td style=text-align:center><code>source_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshreduce-meshreduceop><code>mesh.reduce</code> (mesh::ReduceOp)&nbsp;<a class=headline-hash href=#meshreduce-meshreduceop>¶</a></h3><p><em>Reduce over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.reduce` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              (`reduction` `=` $reduction^)?
              `root` `=` custom&lt;DynamicIndexList&gt;($root_dynamic, $root)
              attr-dict `:` functional-type(operands, results)
</code></pre><p>Reduces on device <code>root</code> within each device group.
<code>root</code> specifies the coordinates of a device along <code>mesh_axes</code>.
It uniquely identifies the root device within its device group.
The accumulation element type is specified by the result type and
it does not need to match the input element type.
The input element is converted to the result element type before
performing the reduction.</p><p>Attributes:
<code>reduction</code>: Indicates the reduction method.</p><p>Example:</p><pre tabindex=0><code>%1 = mesh.reduce %0 on @mesh0 mesh_axes = [1, 0]
  reduction = &lt;max&gt; root = [2, 3]
  : (tensor&lt;3x4xf32&gt;) -&gt; tensor&lt;3x4xf64&gt;
</code></pre><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-7>Attributes:&nbsp;<a class=headline-hash href=#attributes-7>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>reduction</code></td><td>::mlir::mesh::PartialAttr</td><td><details><summary>partial type of a distributed tensor</summary><p>Enum cases:</p><ul><li>sum (<code>Sum</code>)</li><li>max (<code>Max</code>)</li><li>min (<code>Min</code>)</li><li>generic (<code>Generic</code>)</li></ul></details></td></tr><tr><td><code>root</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>ranked tensor of any type values</td></tr><tr><td style=text-align:center><code>root_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshreduce_scatter-meshreducescatterop><code>mesh.reduce_scatter</code> (mesh::ReduceScatterOp)&nbsp;<a class=headline-hash href=#meshreduce_scatter-meshreducescatterop>¶</a></h3><p><em>Reduce-scatter over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.reduce_scatter` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              (`reduction` `=` $reduction^)?
              `scatter_axis` `=` $scatter_axis
              attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>After the reduction, the result is scattered within each device group.
The tensor is split along <code>scatter_axis</code> and the pieces distributed
across the device group.
Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 1, dim_sizes = 2x2)
...
%1 = mesh.reduce_scatter %0 on @mesh0 mesh_axes = [1]
  reduction = &lt;max&gt; scatter_axis = 0
  : tensor&lt;3x4xf32&gt; -&gt; tensor&lt;1x4xf64&gt;
</code></pre><p>Input:</p><pre tabindex=0><code>                          device
                          (0, 1)
                             ↓
                 +-------+-------+  | scatter tensor
device (0, 0) -&gt; |  1  2 |  5  6 |  | axis 0
                 |  3  4 |  7  8 |  ↓
                 +-------+-------+
device (1, 0) -&gt; |  9 10 | 13 14 |
                 | 11 12 | 15 16 |
                 +-------+-------+
                            ↑
                          device
                          (1, 1)
</code></pre><p>Result:</p><pre tabindex=0><code>+-------+
|  6  8 | &lt;- devices (0, 0)
+-------+
| 10 12 | &lt;- devices (0, 1)
+-------+
| 22 24 | &lt;- devices (1, 0)
+-------+
| 26 28 | &lt;- devices (1, 1)
+-------+
</code></pre><p>Traits: SameOperandsAndResultRank</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-8>Attributes:&nbsp;<a class=headline-hash href=#attributes-8>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>reduction</code></td><td>::mlir::mesh::PartialAttr</td><td><details><summary>partial type of a distributed tensor</summary><p>Enum cases:</p><ul><li>sum (<code>Sum</code>)</li><li>max (<code>Max</code>)</li><li>min (<code>Min</code>)</li><li>generic (<code>Generic</code>)</li></ul></details></td></tr><tr><td><code>scatter_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr></table><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshscatter-meshscatterop><code>mesh.scatter</code> (mesh::ScatterOp)&nbsp;<a class=headline-hash href=#meshscatter-meshscatterop>¶</a></h3><p><em>Scatter over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.scatter` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `scatter_axis` `=` $scatter_axis
              `root` `=` custom&lt;DynamicIndexList&gt;($root_dynamic, $root)
              attr-dict `:` functional-type(operands, results)
</code></pre><p>For each device group split the input tensor on the <code>root</code> device along
axis <code>scatter_axis</code> and scatter the parts across the group devices.</p><p>Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 2, dim_sizes = [2, 2])
%1 = mesh.scatter %0 on @mesh0 mesh_axes = [0]
  scatter_axis = 0
  root = [1]
  : (tensor&lt;2x2xi8&gt;) -&gt; tensor&lt;1x2xi8&gt;
</code></pre><p>Input:</p><pre tabindex=0><code>                          device
                          (0, 1)
                             ↓
                 +-------+-------+  | scatter tensor
device (0, 0) -&gt; |       |       |  | axis 0
                 |       |       |  ↓
                 +-------+-------+
device (1, 0) -&gt; |  1  2 |  5  6 |
                 |  3  4 |  7  8 |
                 +-------+-------+
                            ↑
                          device
                          (1, 1)
</code></pre><p>Result:</p><pre tabindex=0><code>                          device
                          (0, 1)
                             ↓
                 +-------+-------+
device (0, 0) -&gt; |  1  2 |  5  6 |
                 +-------+-------+ 
device (1, 0) -&gt; |  3  4 |  7  8 |
                 +-------+-------+
                            ↑
                          device
                          (1, 1)
</code></pre><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-9>Attributes:&nbsp;<a class=headline-hash href=#attributes-9>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>scatter_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td><code>root</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr><tr><td style=text-align:center><code>root_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshsend-meshsendop><code>mesh.send</code> (mesh::SendOp)&nbsp;<a class=headline-hash href=#meshsend-meshsendop>¶</a></h3><p><em>Send over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.send` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `destination` `=` custom&lt;DynamicIndexList&gt;($destination_dynamic, $destination)
              attr-dict `:` functional-type(operands, results)
</code></pre><p>Send from one device to another within a device group.</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-10>Attributes:&nbsp;<a class=headline-hash href=#attributes-10>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>destination</code></td><td>::mlir::DenseI64ArrayAttr</td><td>i64 dense array attribute</td></tr></table><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr><tr><td style=text-align:center><code>destination_dynamic</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=meshshard-meshshardop><code>mesh.shard</code> (mesh::ShardOp)&nbsp;<a class=headline-hash href=#meshshard-meshshardop>¶</a></h3><p><em>Annotate on how a tensor is sharded across a mesh cluster.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.shard` $src `to` $shard (`annotate_for_users` $annotate_for_users^)? attr-dict `:`
              type($result)
</code></pre><p>The mesh.shard operation is designed to specify and guide the sharding
behavior of a tensor value across a mesh topology. This operation has one
operand and two attributes:</p><ol><li><p><code>input</code>: This operand represents the tensor value that needs to be
annotated for sharding.</p></li><li><p><code>shard</code>: This attribute is type of <code>MeshSharding</code>, which is the core data
structure to represent distributed tensor in mesh cluster.</p></li><li><p><code>annotate_for_users</code>: A unit attribute addressing the scenario when a
tensor&rsquo;s sharding annotation differs based on its context of use (either as
a result or an operand). If specified, the sharding pertains to specific
users of the tensor value, indicating how it should be considered when used
as an operand in subsequent operations. If not, the sharding applies to the
operation that defines the tensor value.</p></li></ol><p>Example:</p><pre tabindex=0><code>func.func @only_result_annotated(%arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; : tensor&lt;4x8xf32&gt;
  ...
}

func.func @only_operand_annotated(%arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  ...
}

// The first mesh.shard op applies to %arg0, the second mesh.shard op
// applies for the operand of op0, the third mesh.shard op applies for the
// operand of op2
func.func @both_result_and_multi_operands_annotated(
    %arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; : tensor&lt;4x8xf32&gt;
  %1 = mesh.shard %0 to &lt;@mesh0, [[1]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  %2 = mesh.shard %0 to &lt;@mesh0, [[2]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  &#34;op0&#34;(%1) : ...
  &#34;op1&#34;(%2) : ...
  ...
}
</code></pre><p>The following usages are undefined:</p><pre tabindex=0><code>func.func @annotate_on_same_result_with_different_sharding(
    %arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; : tensor&lt;4x8xf32&gt;
  %1 = mesh.shard %0 to &lt;@mesh0, [[1]]&gt; : tensor&lt;4x8xf32&gt;
  ...
}

func.func @annotate_on_same_result_same_value_with_different_sharding(
    %arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; : tensor&lt;4x8xf32&gt;
  %1 = mesh.shard %arg0 to &lt;@mesh0, [[1]]&gt; : tensor&lt;4x8xf32&gt;
  ...
}

func.func @annotate_on_same_operand_with_different_sharding(
    %arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  %1 = mesh.shard %0 to &lt;@mesh0, [[1]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  ...
}

func.func @result_annotated_after_operand(
    %arg0 : tensor&lt;4x8xf32&gt;) -&gt; () {
  %0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; annotate_for_users : tensor&lt;4x8xf32&gt;
  %1 = mesh.shard %0 to &lt;@mesh0, [[1]]&gt; : tensor&lt;4x8xf32&gt;
  ...
}
</code></pre><p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType</p><p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=attributes-11>Attributes:&nbsp;<a class=headline-hash href=#attributes-11>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>shard</code></td><td>::mlir::mesh::MeshShardingAttr</td><td><details><summary>Attribute that extends tensor type to distributed tensor type.</summary><pre><code>The MeshSharding attribute could be used in the encoding of a
`RankedTensorType` or the mesh.shard op. it contains three sub-attributes:
<ol>
<li>
<p><code>cluster</code>: this attribute is a SymbolRefAttr that refers to the mesh
cluster where the distributed tensor is placed. The symbol must resolve to a
<code>mesh.cluster</code> operation.</p>
</li>
<li>
<p><code>split_axes</code>: is an array composed of int64_t sub-arrays. The outer array&rsquo;s
maximum size is the <code>rank</code> of the related tensor. For the i-th sub-array, if
its value is [x, y], it indicates that the tensor&rsquo;s i-th dimension is splitted
along the x and y axes of the device mesh.</p>
</li>
<li>
<p><code>partial_axes</code>: if not empty, this signifies that the tensor is partial
one along the specified mesh axes. An all-reduce should be applied to obtain
the complete tensor, with reduction type being specified by <code>partial_type</code>.</p>
</li>
<li>
<p><code>partial_type</code>: indicates the reduction type of the possible all-reduce
op. It has 4 possible values:</p>
</li>
</ol>
<ul>
<li><code>partial_sum</code>: denotes it&rsquo;s an all-reduce-sum</li>
<li><code>partial_max</code>: denotes it&rsquo;s an all-reduce-max</li>
<li><code>partial_min</code>: denotes it&rsquo;s an all-reduce-min</li>
<li><code>partial_generic</code>: denotes that the all-reduce type is complex and cannot
be represented merely by a simple sum, max, or min. The exact reduction
computation may be derived from the semantics of the corresponding operation
or from the reduction computation IR</li>
</ul>
<p>Example:</p>
<pre tabindex=0><code>mesh.cluster @mesh0(rank = 3, dim_sizes = [2, 2, 4])

// The tensor is fully replicated on @mesh0.
// Currently, there must be at least one sub-array present in axes, even
// if it&#39;s empty. Otherwise, a parsing error will occur.
tensor&amp;lt;4x8xf32, #mesh.shard&amp;lt;@mesh0, [[]]&amp;gt;&amp;gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0
tensor&amp;lt;4x8xf32, #mesh.shard&amp;lt;@mesh0, [[0]]&amp;gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0 and
// it is also a partial_sum along mesh axis 1.
tensor&amp;lt;4x8xf32, #mesh.shard&amp;lt;@mesh0, [[0], [], [1]]&amp;gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0 and
// it is also a partial_max along mesh axis 1.
tensor&amp;lt;4x8xf32, #mesh.shard&amp;lt;@mesh0, [[0]], partial = max[1]&amp;gt;

// Could be used in the attribute of mesh.shard op
%0 = mesh.shard %arg0 to &amp;lt;@mesh0, [[0]]&amp;gt; : tensor&amp;lt;4x8xf32&amp;gt;
</code></pre><p></code></pre></p></details></td></tr><tr><td><code>annotate_for_users</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>src</code></td><td>Multi-dimensional array with a fixed number of dimensions</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>Multi-dimensional array with a fixed number of dimensions</td></tr></tbody></table><h3 id=meshshift-meshshiftop><code>mesh.shift</code> (mesh::ShiftOp)&nbsp;<a class=headline-hash href=#meshshift-meshshiftop>¶</a></h3><p><em>Sift over a device mesh.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `mesh.shift` $input `on` $mesh (`mesh_axes` `=` $mesh_axes^)?
              `shift_axis` `=` $shift_axis
              `offset` `=` $offset
              (`rotate` $rotate^)?
              attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>Within each device group shift along mesh axis <code>shift_axis</code> by an offset
<code>offset</code>.
The result on devices that do not have a corresponding source is undefined.
<code>shift_axis</code> must be one of <code>mesh_axes</code>.
If the <code>rotate</code> attribute is present,
instead of a shift a rotation is done.</p><p>Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 2, dim_sizes = [2, 4])
%1 = mesh.shift on @mesh0 mesh_axes = [1]
  shift_axis = 1 offset = 2 rotate
  : tensor&lt;2xi8&gt; -&gt; tensor&lt;2xi8&gt;
</code></pre><p>Input:</p><pre tabindex=0><code>mesh axis 1
-----------&gt;

+----+----+----+----+
|  1 |  2 |  3 |  4 |
+----+----+----+----+
|  5 |  6 |  7 |  8 |
+----+----+----+----+
</code></pre><p>Result:</p><pre tabindex=0><code>+----+----+----+----+
|  3 |  4 |  1 |  2 |
+----+----+----+----+
|  7 |  8 |  5 |  6 |
+----+----+----+----+
</code></pre><p>Traits: SameOperandsAndResultElementType, SameOperandsAndResultShape</p><p>Interfaces: SymbolUserOpInterface</p><h4 id=attributes-12>Attributes:&nbsp;<a class=headline-hash href=#attributes-12>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mesh</code></td><td>::mlir::FlatSymbolRefAttr</td><td>flat symbol reference attribute</td></tr><tr><td><code>mesh_axes</code></td><td>::mlir::DenseI16ArrayAttr</td><td>i16 dense array attribute</td></tr><tr><td><code>shift_axis</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td><code>offset</code></td><td>::mlir::IntegerAttr</td><td>64-bit signless integer attribute</td></tr><tr><td><code>rotate</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-11>Operands:&nbsp;<a class=headline-hash href=#operands-11>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>non-0-ranked.tensor of any type values</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h2 id=attributes-13>Attributes&nbsp;<a class=headline-hash href=#attributes-13>¶</a></h2><h3 id=meshshardingattr>MeshShardingAttr&nbsp;<a class=headline-hash href=#meshshardingattr>¶</a></h3><p>Attribute that extends tensor type to distributed tensor type.</p><p>Syntax:</p><pre tabindex=0><code>#mesh.shard&lt;
  ::mlir::SymbolRefAttr,   # cluster
  ::llvm::ArrayRef&lt;::mlir::DenseI32ArrayAttr&gt;,   # split_axes
  ::llvm::ArrayRef&lt;int32_t&gt;,   # partial_axes
  ::mlir::mesh::Partial   # partial_type
&gt;
</code></pre><p>The MeshSharding attribute could be used in the encoding of a
<code>RankedTensorType</code> or the mesh.shard op. it contains three sub-attributes:</p><ol><li><p><code>cluster</code>: this attribute is a SymbolRefAttr that refers to the mesh
cluster where the distributed tensor is placed. The symbol must resolve to a
<code>mesh.cluster</code> operation.</p></li><li><p><code>split_axes</code>: is an array composed of int64_t sub-arrays. The outer array&rsquo;s
maximum size is the <code>rank</code> of the related tensor. For the i-th sub-array, if
its value is [x, y], it indicates that the tensor&rsquo;s i-th dimension is splitted
along the x and y axes of the device mesh.</p></li><li><p><code>partial_axes</code>: if not empty, this signifies that the tensor is partial
one along the specified mesh axes. An all-reduce should be applied to obtain
the complete tensor, with reduction type being specified by <code>partial_type</code>.</p></li><li><p><code>partial_type</code>: indicates the reduction type of the possible all-reduce
op. It has 4 possible values:</p></li></ol><ul><li><code>partial_sum</code>: denotes it&rsquo;s an all-reduce-sum</li><li><code>partial_max</code>: denotes it&rsquo;s an all-reduce-max</li><li><code>partial_min</code>: denotes it&rsquo;s an all-reduce-min</li><li><code>partial_generic</code>: denotes that the all-reduce type is complex and cannot
be represented merely by a simple sum, max, or min. The exact reduction
computation may be derived from the semantics of the corresponding operation
or from the reduction computation IR</li></ul><p>Example:</p><pre tabindex=0><code>mesh.cluster @mesh0(rank = 3, dim_sizes = [2, 2, 4])

// The tensor is fully replicated on @mesh0.
// Currently, there must be at least one sub-array present in axes, even
// if it&#39;s empty. Otherwise, a parsing error will occur.
tensor&lt;4x8xf32, #mesh.shard&lt;@mesh0, [[]]&gt;&gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0
tensor&lt;4x8xf32, #mesh.shard&lt;@mesh0, [[0]]&gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0 and
// it is also a partial_sum along mesh axis 1.
tensor&lt;4x8xf32, #mesh.shard&lt;@mesh0, [[0], [], [1]]&gt;

// The tensor is sharded on the first dimension along axis 0 of @mesh0 and
// it is also a partial_max along mesh axis 1.
tensor&lt;4x8xf32, #mesh.shard&lt;@mesh0, [[0]], partial = max[1]&gt;

// Could be used in the attribute of mesh.shard op
%0 = mesh.shard %arg0 to &lt;@mesh0, [[0]]&gt; : tensor&lt;4x8xf32&gt;
</code></pre><h4 id=parameters>Parameters:&nbsp;<a class=headline-hash href=#parameters>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>cluster</td><td style=text-align:center><code>::mlir::SymbolRefAttr</code></td><td>cluster placed</td></tr><tr><td style=text-align:center>split_axes</td><td style=text-align:center><code>::llvm::ArrayRef&lt;::mlir::DenseI32ArrayAttr></code></td><td></td></tr><tr><td style=text-align:center>partial_axes</td><td style=text-align:center><code>::llvm::ArrayRef&lt;int32_t></code></td><td></td></tr><tr><td style=text-align:center>partial_type</td><td style=text-align:center><code>::mlir::mesh::Partial</code></td><td></td></tr></tbody></table><h3 id=partialattr>PartialAttr&nbsp;<a class=headline-hash href=#partialattr>¶</a></h3><p>partial type of a distributed tensor</p><p>Syntax:</p><pre tabindex=0><code>#mesh.partial&lt;
  ::mlir::mesh::Partial   # value
&gt;
</code></pre><p>Enum cases:</p><ul><li>sum (<code>Sum</code>)</li><li>max (<code>Max</code>)</li><li>min (<code>Min</code>)</li><li>generic (<code>Generic</code>)</li></ul><h4 id=parameters-1>Parameters:&nbsp;<a class=headline-hash href=#parameters-1>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::mesh::Partial</code></td><td>an enum of type Partial</td></tr></tbody></table><h2 id=attributes-14>Attributes&nbsp;<a class=headline-hash href=#attributes-14>¶</a></h2><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Dialects/MemRef/ title="'memref' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - 'memref' Dialect</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/ title="'ml_program' Dialect">Next - 'ml_program' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/deprecation/TestingGuide/>Testing Guide haha</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/BufferDeallocationInternals/>Buffer Deallocation - Internals</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li class=active><a href=https://mlir.llvm.org/docs/Dialects/Mesh/>'mesh' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>